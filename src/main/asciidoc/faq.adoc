= FAQ

== Super-fast commit

Fim is able to commit using the super-fast mode (or fast mode). In that case, it checks into the repository for modifications using the selected mode. +
And then it do a full hash of the modified files in order to fill up the missing hash. +
*Using Super-fast commit allow to save a lot of time.* With this option you can miss some modified files.

This means that even if the global hash mode of your repository is hash all, you can commit using either fast mode or super-fast mode.

Super-fast commit example:

[source, bash]
------
$ fim init -y -c "Create the repository slowly in full mode"
Info  - Scanning recursively local files, using 'full' mode and 2 threads
...

# Do some modifications

$ fim ci -s -y -c "Commit modifications very quickly using super-fast commit"
Info  - Scanning recursively local files, using 'super-fast' mode and 2 threads
...
Info  - Retrieving the missing hash for all the modified files, using 'full' mode and 2 threads
...

# Do some modifications

$ fim ci -f -y -c "Commit modifications quickly using fast commit"
Info  - Scanning recursively local files, using 'fast' mode and 2 threads
...
Info  - Retrieving the missing hash for all the modified files, using 'full' mode and 2 threads
...
------

== Run Fim commands from a sub-directory

You can run all the Fim commands from every sub-directory inside a Fim repository.
You can see it in action into the <<simple-example.adoc#_from_the_code_dir01_code_sub_directory,Simple example>>.

Doing this allow you to:

- Quickly find the modifications done in this specific sub-directory. You will hash only the files contained inside and not the complete file tree
- Quickly commit the modifications done in this sub-directory
- Quickly find the duplicated files contained in this sub-directory
- Quickly reset the attributes of files contained in this sub-directory

All the other commands will run as if you were on the top of the Fim repository.

== Dealing with duplicates

Duplicated files are addressed by Fim in two different ways.

=== Duplicates inside a Fim repository

Fim allow you to detect duplicates using the `fdup` command. It displays the list of duplicated files. +
See it in action in <<simple-example.adoc#_search_for_duplicated_files,Search for duplicated files>>.

If you want to remove them, Fim won't do it. It does not provide a smart way to remove those duplicates.

=== Duplicates that are outside

You can use Fim to remove duplicated files that are located outside a Fim repository using the `rdup` command.
It can be useful if you want to cleanup old backups that are no more synchronized and you want to be sure to not lose any files that could have been modified or added.

==== Simple example

$ cd /tmp
/tmp$ mkdir source
/tmp$ cd source/
/tmp/source$ for i in 01 02 03 04 05 06 07 08 09 10 ; do echo "New File $i" > file$i ; done
/tmp/source$ ll
total 48K
drwxrwxr-x  2 evrignaud evrignaud 4,0K mai   21 08:31 .
drwxrwxrwt 17 root      root      4,0K mai   21 08:31 ..
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file01
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file02
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file03
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file04
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file05
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file06
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file07
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file08
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file09
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file10
/tmp/source$ fim init -y
No comment provided. You are going to initialize your repository using the default comment.
2016/05/21 08:32:07 - Info  - Scanning recursively local files, using 'full' mode and 4 threads
(Hash progress legend for files grouped 10 by 10: # > 1 GB, @ > 200 MB, O > 100 MB, 8 > 50 MB, o > 20 MB, . otherwise)
.
2016/05/21 08:32:07 - Info  - Scanned 10 files (120 bytes), hashed 120 bytes (avg 120 bytes/s), during 00:00:00

Added:            file01
Added:            file02
Added:            file03
Added:            file04
Added:            file05
Added:            file06
Added:            file07
Added:            file08
Added:            file09
Added:            file10

10 added
/tmp/source$ cd ..
/tmp$ cp -a source backup
/tmp$ cd source/
/tmp/source$ ll
total 52K
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:32 .
drwxrwxrwt 18 root      root      4,0K mai   21 08:32 ..
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file01
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file02
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file03
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file04
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file05
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file06
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file07
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file08
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file09
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file10
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:32 .fim
/tmp/source$ echo modif1 >> file02
/tmp/source$ echo modif2 >> file04
/tmp/source$ ll
total 52K
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:32 .
drwxrwxrwt 18 root      root      4,0K mai   21 08:32 ..
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file01
-rw-rw-r--  1 evrignaud evrignaud   19 mai   21 08:32 file02
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file03
-rw-rw-r--  1 evrignaud evrignaud   19 mai   21 08:32 file04
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file05
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file06
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file07
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file08
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file09
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file10
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:32 .fim
/tmp/source$ fim diff -s
2016/05/21 08:32:58 - Info  - Scanning recursively local files, using 'super-fast' mode and 4 threads
(Hash progress legend for files grouped 10 by 10: # > 1 GB, @ > 200 MB, O > 100 MB, 8 > 50 MB, o > 20 MB, . otherwise)
.
2016/05/21 08:32:58 - Info  - Scanned 10 files (134 bytes), hashed 134 bytes (avg 134 bytes/s), during 00:00:00

Comparing with the last committed state from 2016/05/21 08:32:07
Comment: Initial State

Content modified: file02 	creationTime: 2016/05/21 08:31:53 -> 2016/05/21 08:32:45
                         	lastModified: 2016/05/21 08:31:53 -> 2016/05/21 08:32:45

Content modified: file04 	creationTime: 2016/05/21 08:31:53 -> 2016/05/21 08:32:51
                         	lastModified: 2016/05/21 08:31:53 -> 2016/05/21 08:32:51


2 content modified
/tmp/source$ fim ci -s -c "Modified 2 files"
2016/05/21 08:33:18 - Info  - Scanning recursively local files, using 'super-fast' mode and 4 threads
(Hash progress legend for files grouped 10 by 10: # > 1 GB, @ > 200 MB, O > 100 MB, 8 > 50 MB, o > 20 MB, . otherwise)
.
2016/05/21 08:33:18 - Info  - Scanned 10 files (134 bytes), hashed 134 bytes (avg 134 bytes/s), during 00:00:00

Comparing with the last committed state from 2016/05/21 08:32:07
Comment: Initial State

Content modified: file02 	creationTime: 2016/05/21 08:31:53 -> 2016/05/21 08:32:45
                         	lastModified: 2016/05/21 08:31:53 -> 2016/05/21 08:32:45

Content modified: file04 	creationTime: 2016/05/21 08:31:53 -> 2016/05/21 08:32:51
                         	lastModified: 2016/05/21 08:31:53 -> 2016/05/21 08:32:51


2 content modified

Do you really want to commit (y/n/A)? y
2016/05/21 08:33:20 - Info  - Retrieving the missing hash for all the modified files, using 'full' mode and 4 threads
2016/05/21 08:33:20 - Info  - Scanned 2 files (38 bytes), hashed 38 bytes (avg 38 bytes/s), during 00:00:00

/tmp/source$ cd ../backup/
/tmp/backup$ fim rdup -m ../source
2016/05/21 08:33:45 - Info  - Searching for duplicated files using the ../source directory as master

2016/05/21 08:33:45 - Info  - Scanning recursively local files, using 'full' mode and 4 threads
(Hash progress legend for files grouped 10 by 10: # > 1 GB, @ > 200 MB, O > 100 MB, 8 > 50 MB, o > 20 MB, . otherwise)
.
2016/05/21 08:33:46 - Info  - Scanned 10 files (120 bytes), hashed 120 bytes (avg 120 bytes/s), during 00:00:00

'file01' is a duplicate of '../source/file01'
Do you really want to remove it (y/n/A)? y
  'file01' removed
'file03' is a duplicate of '../source/file03'
Do you really want to remove it (y/n/A)? y
  'file03' removed
'file05' is a duplicate of '../source/file05'
Do you really want to remove it (y/n/A)? y
  'file05' removed
'file06' is a duplicate of '../source/file06'
Do you really want to remove it (y/n/A)? y
  'file06' removed
'file07' is a duplicate of '../source/file07'
Do you really want to remove it (y/n/A)? y
  'file07' removed
'file08' is a duplicate of '../source/file08'
Do you really want to remove it (y/n/A)? y
  'file08' removed
'file09' is a duplicate of '../source/file09'
Do you really want to remove it (y/n/A)? y
  'file09' removed
'file10' is a duplicate of '../source/file10'
Do you really want to remove it (y/n/A)? y
  'file10' removed

8 duplicated files found. 8 duplicated files removed
/tmp/backup$ ll
total 20K
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:33 .
drwxrwxrwt 18 root      root      4,0K mai   21 08:33 ..
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file02
-rw-rw-r--  1 evrignaud evrignaud   12 mai   21 08:31 file04
drwxrwxr-x  3 evrignaud evrignaud 4,0K mai   21 08:32 .fim


==== Complex cases

Let say that you have:

* a directory with a big file tree that we will call the source location.
* other locations that contains some files that was copied long ago from this source location. We will call one those locations the backup location.

Now you want to cleanup the backup location from the files that are identical with the ones in the source location.
To find duplicates into the backup location we will use the hash located into the source `.fim` directory.
We will call master location the name of the directory where is this `.fim`. +
**Most of the time the master location is the source location.** +
If the source location is not reachable from the backup location, you just need to put a copy of the source `.fim` directory near the backup location.

[NOTE]
====
The backup location can contain also his own `.fim` directory. It will be ignored.
====

===== Step by step

* Go into the source location and ensure that all the hash are up to date:

[source, bash]
----
$ cd <source location>
$ fim ci -y -c "Content added"
----

* If the backup location cannot reach the source location (so master location is not the source location),
copy the `.fim` directory that is in the source location into a place near the backup location.

[source, bash]
----
$ cd <somewhere>
$ mkdir <master location>
$ scp -rp <remote host>@<source location>/.fim <master location>
----

[IMPORTANT]
====
The source `.fim` directory can't be nested into the root folder of the backup location.
====

* Run the remove duplicates command. For this, go in the backup location.

[source, bash]
----
$ cd <backup location>
$ fim rdup -m <master location>
----

== Changing default hash mode

If you never want to hash the complete content of your files you can set a global hash mode that will indicate the maximum hash mode you want to use for this repository.
You can specify this to the init command:

- `-f`: Sets maximum hash mode to fast. You will be able to use `-f`, `-s` or `-n` after
- `-s`: Sets maximum hash mode to super-fast. You will be able to use `-s` or `-n` after
- `-n`: Means always don't hash anything. You won't be able to use other hash mode after

=== Example

Initialize the Fim repository specifying the global hash mode.

[source, bash]
----
$ fim init -f
----

It sets a global hash mode for the complete repository to fast mode. +
All the Fim commands that you use after will use by default the fast hash mode (or less if specified) and you won't be able to hash the full file contents.

After the init command that we run in our example, you will be able to run the following commands:

[source, bash]
----
$ fim diff    # will run using -f

$ fim diff -s

$ fim diff -n
----

== Ignoring files or directories

You can specify files or directories that you want to be ignored by Fim.
For this, you can add a `.fimignore` file in one of the directories contained into the Fim repository. +
You can also set global ignores by creating a `.fimignore` into the user home directory.

Each line of the `.fimignore` file specifies a pattern. The pattern is mainly a file or directory name. +
Use wildcards in order to match many of them. For example `\*.mp3` will match all the files ending with `.mp3`. +
A leading `*\*` followed by a slash means match in all directories. +
For example, `**/foo` matches a file or a directory named `foo` anywhere, starting from where the `.fimignore` contain this pattern.

== Hash files in multi-thread

Fim hashes files using several threads.
This allows taking advantage of the computer resources and maximizes the overall performances of file hashing. +
By default, Fim use an arbitrary thread count that is the number of CPU core divided by two. +
You can specify, using the `-t` option, the number of thread to be used for file hashing. +
The best value depends on the kind of hard disk you have. The more throughputs you have, the more thread you can use.

== State integrity

Every State file contains a hash of the State content. +
If something is modified in the State file, the hash of the State content will change and the State will be reported as corrupted. +
Fim won't use a corrupted State.

== Cross platform compatibility

The same Fim repository can be used by either on Linux, Mac OS X and Windows. +
State content is normalized and the same State content can be loaded on the different supported OS.
